\section{Recognition and pose estimation pipeline} \label{sec:linemod-pipeline}

In this chapter a pipeline similar to the one introduced in
\cite{linemod-pipeline} is presented, which is used to detect and estimate
the pose of the objects to be recognized. First a template matching approach is
used to detect the objects, then matches are progressively excluded (false
positives detection) based on hue values; finally valid matches are
progressively refined by physically moving a precision depth camera and
performing depth-based alignment of the match and the real object.

\subsection{Image matching using the Line-MOD algorithm} \label{sec:match-with-linemod}
The proposed object recognition algorithm is started by an upper
algorithm, managing the whole gripping strategy, which requires to the
recognition system to find a set of known objects into an RGBD
image. Three images are actually passed for recognition, which are
assumed to be registered at a pixel level: the RGB image of the scene,
the corresponding depth image, and a mask. The latter is an 8-bit
image with the same size of the whole scene, having each pixel with
value 255 if the pixel must be evaluated for recognition, and 1 if it
must be discarded.

For each object, the recognition system can be instructed to use a
recognition algorithm different from Line-MOD: if it isn't, the object
is assumed to correspond to a valid object for the Line-MOD
training database. This is the case which is treated in the rest of
this section.

For each model which has to be recognized using Line-MOD, the
corresponding training database is read from memory; this database
is created at initialization time from a directory tree which
associates each object's ID with the corresponding detector
description (number of modalities, type of modalities, mesh used for
rendering the object, etc.) and with the list of templates which are
associated to the object. After reading the objects, all of their
templates are merged together to form a single template list which
will be used for matching: information about the nature of each
template is mantained inside the template itself, as each item of the
list contains both the template's features and some metadata, like the
object ID the template refers to, the pose of the object which
corresponds to it, and additional informations which can were saved
during training (for example, if a simple filtering algorithm for false
positives  based on RGB histograms has to be implemented, it is enough
to link these data to the templates when training, and they will be
easily recovered when matching); the features of the image which is
included into the input mask are then computed, their binary
representation and response map are generated as described in
sec.~\ref{sec:linemod-binary}, and each template is matched upon
this. As the most intensive part for Line-MOD is the computation of
the reponse map for the scene's image, while template matching is
almost istantaneous, it is very convenient to match all the possible
templates in a single pass, as this will require the scene image to be
processed only once.

For each match, a corresponding matching percentage is defined. Given
the similarity function of eqn.~\ref{eqn:similarity-function}, the
percentage $S_\%$ of match for each template $T$ can be easily defining by noting
that the maximum possible similarity score, given when the two
images match perfectly, is given by the similarity function of the
scene's section $I$ with itself:

\begin{equation} \label{eqn:match-percentage}
  S_\%(T) = \frac{S(I,T)}{S_max(I)} = \frac{S(I,T)}{\sum_{p \in I}
    \left( \tau I_{p} \right)}
\end{equation}

In order to avoid considering matches which bring too little
information due to the extremely low matching area, a filter is put on
all the matches that come out from this cycle, checking both the
matching percentage defined in eqn.~\ref{match-percentage} to be over
a threshold $t_\%$ and the total area (in pixels) of the template
which matched to be over a threshold $t_{\text{area}}$. All the
matches that pass this check are kept as valid and stored into a
match list.

When this loop exits, thus, it will output a list of matches which are
tolerably strong and matched with a valid object pose and ID.

As it operates on multiple objects at the same time, this algorithm
will make sure to find at least a valid match for each object before
exiting; if this does not happen at the first iteration, the
Line-MOD algorithm is started again, and the minimum similarity
threshold $t_\%$ is decreased by a factor 0.95; matches (which can
uniquely identified by the object ID, template ID, and matching
coordinates $(u,v)$) which have already been processed will be
discarded in the following loops in order not to count them twice.

\subsection{False positive rejection based on hue values}
The first step of pose matching is made by purpose to be overly tolerant in
template matching. This is because the used objects are quite small and simple
in shape - which can reduce the number of features to match - and at the first
step are seen from far away ( more than 1 meter ) in order to maximize the
field of view without increasing the needed number of cameras. The counterpart
for being sure to match \emph{at least} something is having a huge number of
  false positives, which must be recognized and excluded before starting the
  alignment.


To do this, after performing each match together with a rough pose estimation,
the object is rendered again into its estimated position, and the portion of
source image corresponding to the render is taken. The two images are then
compared on a per-pixel basis, considering only the pixels that lie on the
depth mask created during rendering. 

Before comparing, the two images are converted to the HSV colour space, and
comparation is done based on the hue values. This allows to only consider the
actual colour of the model and match, and discard all information about
surface behaviour with respect to light, together with variations in the images
depending on different lightning conditions.

HSV colour space has singularities on the hue dimension when the chosen pixel is
either black or white; in this case, in effect, the hue of the pixel is
undefined due to the low saturation or low luminosity (\emph{value}). To fix
this, white and black pixels are turned to be zero-saturated, zero-valued
pixels with fixed hue, corresponding to yellow (for white) and blue (for
black). When matching, these are treated as special values and two pixels are
said to match either if their actual hue match (which
is robust to moderate lightning changes) or if their modified hue, saturation
and values match
(which happes if both the model and the scene appear to be actually white or
black). 

After computing the new HSV values, the four resulting images (two belonging to
the source image, two belonging to the rendered one) are scaled down of a
factor $K_s$. This makes the four images to be downsampled, and improves the
success rate of the subsequent step as it does minimize small alignment errors
due to the pose estimation error within the template. The render's mask $m$ is
scaled down of the same factor.

After scaling, the mask is first filtered in order to keep only the points for
which the mask's value is 1 (thus correcting the interpolated values on the
border which come out of the scaling algorithm) and then
eroded by a small amount (1 pixel). This allows to further remove the noise
which usually comes into the object's borders.

Finally, for each pixel belonging to the newly created mask, a comparison if
done between the corresponding render's and scene's pixel. If their difference
is below a certain threshold $t$, the two pixels correspond. The corresponding
pixels are counted and the matching ratio $P_{match}$ is computed with respect to the
total number of pixels belonging to the scaled mask:

\begin{equation}
P_{match}=\frac{\text{count}\left[(u,v) : m_{u,v}=1 \wedge \left( \lvert
\text{Hue}(r_{u,v})-\text{Hue}(s_{u,v})\rvert < t \vee R_{u,v}=S_{u,v} \right)
\right]
}{\text{count}\left[ (u,v) : m_{u,v}=1 \right] }
\end{equation}

Here, $r$ indicates the scaled-down render, $s$ indicates the scaled-down scene
portion, while $R$ and $S$ indicate the corresponding images with modified hues.

The whole matching and filtering part is repeated until at least one match
passes the filtering step. If no valid matches are found during the first
iteration, the threshold level for the LineMOD algorithm is decreased
just like it was done in the previous step of the pipeline; in this
way, objects with small, partial occlusions can be recognized.

If the threshold $t_\%$ falls below a threshold $t_{\%,\text{bad}}$
and no valid matches have still been found to pass this check for every object, however, the
algorithm exits anyway: exeperimental tests have, in fact, shown that
when the matching threshold becomes below about $80\%$ the algorithm
thends to recognize almost every template as matching, but it will
still fail to recognize correctly the object in its correct
position. This will lead to a huge number of false positives (about
$50\%$) of the total templates, and the hue matching step described in
this section will in turn last several minutes just to continue
failing, which is a situation to avoid. 

\subsection{Point cloud alignments and pose refinement}
The matches that passed the first false-positive rejection step are saved and
sorted based onto their matching percentage. At this point, matches are probably
valid, but only colour information has been considered. Also, a good pose
approximation has been obtained for each match, but coming from a template, it
is not precise enough to be used for grasping. To overcome these problems,
another step is required for each match, which is pose refinement through point
clouds' alignment. In this step, colour information is dropped, and only depth
information is used. 

The Microsoft Kinect RGBD camera provides good depth informations for the template-matching
requirements, but it is not suitable for precisely find the pose of an object
for grasping. This is due mainly to the limitations of its depth sensor, having
a nonlinear depth error of about $1\%$\footnote{TODO:link at MS device page}.
Although this is quite a standard error, it must be combined with the minimum
depth range of $80\unit{cm}$ for the sensor. This leads to an error of about
$1\unit{cm}$ on the depth's values. Also, the field of view of the (fixed)
kinect does not allow to precisely detect the object's pose, as the depth error
increases the more the obejct is not centered with respect to the sensor. As a
higher precision is needed for good 
object grasping, a dedicated, precision depth sensor (\emph{Structure Sensor by
Occipital})has been mounted onto the robot's arm.


This camera's specifications\footnote{http://structure.io/developers} are much better than the kinect's ones, although it
does not provide RGBD images but only depth informations -- hence the need for two
different cameras; it has a much lower minimum range of $40\unit{cm}$ and is suitable for
precision depth measurements, having a maximum error of about $1\%$, which comes
down to $0.15\%$ at $40\unit{cm}$ of distance. This means having an absolute
error of less than $0.2\unit{mm}$ when the camera is close enough to the object.

As the matches already provided a good estimation of the object's position, the
iterative closest point algorithm explained into sec. \ref{sec:icp} can be
applied. 

First, a frame can be taken
First, the point cloud %TODO
for the scene is obtained as explained in sec. \ref{sec:cloud-reconstruction}.
As the mask of the render has already been computed, it is exploited at this
point and only the part of the scene corresponding to the actual object is
transformed to a 3D point cloud.

During the process of 3D reconstruction a lot of invalid values can be
generated: these are the points for which the depth sensor failed to obtain a
valid representation
